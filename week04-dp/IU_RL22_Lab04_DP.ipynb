{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Recap for the MDP lab\n",
        "### Markov Decision Processes\n",
        "\n",
        "Let's formalize the key components of the RL problem in the context of MDPs:\n",
        "\n",
        "An MDP is defined by: $(S, A, P, R, S_0, \\gamma)$\n",
        "* S = set of states (state-space)\n",
        "* A = set of actions (action-space)\n",
        "* P = state transition probabilities\n",
        "* R = reward for taking an action $a\\in\\text{A}$ in state $s\\in\\text{S}$\n",
        "* $S_0$ = starting state\n",
        "* $\\gamma$ = discount rate\n",
        "    \n",
        "In more detail:\n",
        "* **States** - states can be discrete/finite (imagine cells in a grid world) or continuous/infinite (position on a road).\n",
        "    * Referred to as the *state space* (i.e. discrete state space or continuous state space)\n",
        "* **Actions** - actions can also be discrete (moving up/down/left/right in a grid world cell) or continuous (how many degrees to turn a steering wheel when driving a car).\n",
        "    * Referred to as the *action space* (i.e. discrete action space or continuous action space)\n",
        "* **Rewards** - rewards are issued by a reward function $\\rho : S_t \\times A_t \\rightarrow R$. The reward function is a property of the environment.\n",
        "* **Transition probabilities**. In MDPs, this is denoted by $P_{s,a}$. The transition probability is the probability that, for example, some action $A$ in state $S$ leads to state $S^\\prime$ (prime denotes the next time step) - represented notationally as $P(s^\\prime|s,a)$.\n",
        "* **Discount factor** - the discount factor is a number greater than 0 and less than 1 that is used to discount rewards received over sequential time-steps. It is denoted as $\\gamma \\in [0, 1)$\n",
        "* **Value function** - one of the primary functions learned by the agent: the value function dictates either the value of a state or the value of action. More on this below.\n",
        "* **Policy function** - one of the primary functions learned by the agent: the policy maps states to actions. More below.\n",
        "\n",
        "#### Other useful definitions\n",
        "* **Experience** - $\\big(\\text{State}_{t}$, $\\text{Action}_{t}$, $\\text{Reward}_{t}\\big)$ tuple\n",
        "* **Trajectory** - A sequence of *experiences* through time, represented as: $\\tau$ (tau)\n",
        "* **Episode** - A trajectory that ends in a terminal state\n",
        "\n",
        "\n",
        "### Policy\n",
        "\n",
        "The process of learning for the agent can be thought of as a sequence of mapping states to actions $a = \\pi(s)$ to maximize expected reward over an episode. This is known as the **policy:** $\\pi: S \\rightarrow A$. \n",
        "\n",
        "Note:\n",
        "* The agent needs to explore and interact with its environment to learn where actions earn the maximum rewards.\n",
        "* Actions in the current time step effect rewards in future time steps\n",
        "* There is a trade off between the frequency of sampling the environment and frequency of taking actions\n",
        "* It can be based on discrete state-spaces or continuous state-spaces (and same for action-spaces)\n",
        "\n",
        "### Value functions\n",
        "\n",
        "The **worthiness** of a policy is calculated by the aforementioned *value function*. There are various forms of value functions. First, the value of a state:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1W6LE42KbtNoOs1LKkFxlWhOF6-7wzWxS\" />\n",
        "\n",
        "Second, the value of action:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1I0Df6jgFDH7gBU1djylJ8ZzmJUO36P9_\" />\n",
        "\n",
        "\n",
        "\n",
        "#### <font color=\"#DE008A\">Bellman's optimal ($*$) action-value function</font> :\n",
        "\n",
        "$$Q^*{(s,a)} = \\ R{(s,a)} + \\gamma \\max_{a^\\prime} \\sum_{s^\\prime\\in\\text{S}} \\big[P{(s^\\prime|s,a)}  Q^*{(s^\\prime,a^\\prime)}\\big]$$\n"
      ],
      "metadata": {
        "id": "Aheo3nJp1g3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning Robot Problem\n",
        "\n",
        "The main characteristics of this world are the following:\n",
        "\n",
        "- Discrete time and space\n",
        "- Fully observable\n",
        "- Infinite horizon\n",
        "- Known Transition Model\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1Yz6xnDuo6StlKzmj4eDqVYvZfOORBlBT\"/>\n",
        "\n",
        "The main goal for the robot in this task is to find the best way to reach the charging station.\n",
        "\n",
        "What does the \"best way\" even mean?\n",
        "It depends on the reward that the robot receives in each intermediate state -> that leads to multiple optimal policies.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1vef0Xhpy5OMwTxKLgBBqxrxNTp9bhw9S\"/>\n",
        "\n",
        "**The Bellman equation**\n",
        "\n",
        "$$Q^*{(s,a)} = \\ R{(s,a)} + \\gamma \\max_{a^\\prime} \\sum_{s^\\prime\\in\\text{S}} \\big[P{(s^\\prime|s,a)}  Q^*{(s^\\prime,a^\\prime)}\\big]$$\n",
        "\n",
        "In this example the reward for each non-terminal state is \n",
        "$$R(s) = −0.04$$\n",
        "\n",
        "\n",
        "BUT, before we begin let's assume that somehow we got the optima optimal policy and utility values generated by the optimal value-function just to help us understand the idea.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1C-hx0MwCi9hNQcM9Q-QIvVJzalbVWypa\"/>\n",
        "\n",
        "In our example we suppose the robot starts from the state (1,1). Using the Bellman equation we have to find the action with the highest utility between UP, LEFT, DOWN and RIGHT. We do not have the optimal policy, but we have the transition model and the utility values for each state. You have to recall the two main rules of our environment: (i) if the robot bounce on the wall it goes back to the previous state, and (ii) the selected action is executed only with a probability of 80% in accordance with the transition model. Instead of dealing with those ugly numbers I want to show you a visual representaion of the possible outcomes:\n",
        "<img src=\"https://drive.google.com/uc?id=1_X65joHfWYkFNEIorjSNXmMR0rRGNTjX\"/>\n",
        "\n",
        "For each possible outcome I reported the utility and the probability given by the transition model. This corresponds to the first part of the Bellman equation. The next step is to calculate the product between the utility and the transition probability, then sum up the value for each action.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1lmDRxIXOs0h6cGP6VzzsRjauu-nGJYbO\"/>\n",
        "\n",
        "We found out that for state (1,1) the action UP has the highest value. This is in accordance with the optimal policy we magically got.\n",
        "\n",
        "Now we have all the elements and we can plug the values in the Bellman equation finding the utility of the state (1,1):\n",
        "\n",
        "$$U(s11)=−0.04 + 1.0 × 0.7456= 0.7056$$"
      ],
      "metadata": {
        "id": "Eh-qQmfCJVOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Dynamic Programming (DP)"
      ],
      "metadata": {
        "id": "5qbUGWMp15L0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dynamic programming (DP) is used to compute optimal policies given a complete model of the environment\n",
        "  - This model should be a Markov Decision Process (MDP)\n",
        "  - In dynamic programming, the dynamics function, $P$ should be known\n",
        "    - i.e $ P(s'|s, a)$ should be fully defined\n",
        "    - What do we mean by this?\n",
        "    - Well, basically when given a state and action, the model of the environment will give the probabilities for all possible next states and rewards \n"
      ],
      "metadata": {
        "id": "A7EMeZcA0i5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In DP (Dynamic programming) the objective is to determine optimal policies using optimal value functions\n",
        "  - In order to do that, we first calculate optimal value functions for each state \n",
        "  - As a recap: value functions allow us to get an idea of how good each state is, in terms of giving highest accumulated reward\n",
        "    - More formally, state-value functions are the \"expected\" future rewards starting at some state, $s$ and then following a specific policy\n",
        "   - Action-value functions are the \"expected\" future rewards starting at some state $s$ and action $a$ and then following a specific policy\n",
        "- So, that leads us to one question: how do we determine state-values and action-values?"
      ],
      "metadata": {
        "id": "50xKvwhM2W1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The policy iteration algorithm"
      ],
      "metadata": {
        "id": "5G0JYRSB7F49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "use the policy iteration algorithm to find an optimal policy that maximizes the expected reward. No policy generates more reward than the optimal policy $\\pi^∗$\n",
        ". Policy iteration is guaranteed to converge and at convergence, the current policy and its utility function are the optimal policy and the optimal utility function. First of all, we define a policy \n",
        "$\\pi$ assigning an action to each state. We can assign random actions to this policy, it does not matter. Using the Bellman equation we can compute the expected utility of the policy. There is a good news. We do not really need the complete version of the Bellman equation which is:\n",
        "\n",
        "$$Q{(s,a)} = \\ R{(s,a)} + \\gamma \\color{red}{\\max_{a^\\prime}} \\sum_{s^\\prime\\in\\text{S}} \\big[P{(s^\\prime|s,a)}  Q{(s^\\prime,a^\\prime)}\\big]$$\n",
        "\n",
        "Since we have a policy and the policy associate to each state an action, we can get rid of the \n",
        " max \n",
        " operator and use a simplified version of the Bellman equation:\n",
        "\n",
        " $$Q{(s,a)} = \\ R{(s,a)} + \\gamma \\sum_{s^\\prime\\in\\text{S}} \\big[P{(s^\\prime|s,a)}  Q{(s^\\prime,\\pi(s))}\\big]$$\n"
      ],
      "metadata": {
        "id": "kt5HxAa77Vj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have evaluated the policy, we can improve it. Policy improvement is the second and last step of the algorithm. Our environment has a finite number of states, therefore a finite number of policies. Each iteration returns a better policy."
      ],
      "metadata": {
        "id": "WcNx8O1FHduv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/mhd-medfa/IU-Reinforcement-Learning-22-lab/raw/main/week03-mdp/T.npy  -O T.npy"
      ],
      "metadata": {
        "id": "li9qwo6mVi8_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MDP:\n",
        "  def __init__(self):\n",
        "        #Starting state vector\n",
        "        #The agent starts from (1, 1)\n",
        "        self.states = np.array([[0.0, 0.0, 0.0, 0.0, \n",
        "                                    0.0, 0.0, 0.0, 0.0, \n",
        "                                    1.0, 0.0, 0.0, 0.0]])\n",
        "        self.rewards = np.array([-0.04, -0.04, -0.04,  +1.0,\n",
        "                                 -0.04,   0.0, -0.04,  -1.0,\n",
        "                                 -0.04, -0.04, -0.04, -0.04])\n",
        "        \n",
        "        # Probabilities Transition matrix loaded from file\n",
        "        #(It is too big to write here)\n",
        "        self.transits = np.load(\"T.npy\")\n",
        "        #Generate the first policy randomly\n",
        "        # Nan=Nothing, -1=Terminal, 0=Up, 1=Left, 2=Down, 3=Right\n",
        "        self.policy = np.random.randint(0, 4, size=(12)).astype(np.float32)\n",
        "        self.policy[5] = np.NaN\n",
        "        self.policy[3] = self.policy[7] = -1\n",
        "\n",
        "        #Utility vector\n",
        "        self.values = np.array([0.0, 0.0, 0.0,  0.0,\n",
        "                                0.0, 0.0, 0.0,  0.0,\n",
        "                                0.0, 0.0, 0.0,  0.0])\n",
        "        self.gamma = 0.999\n",
        "\n",
        "        self.epsilon = 0.0001\n",
        "        self.iteration = 0\n",
        "\n",
        "  def policy_evaluation(self, shape=(3,4)):\n",
        "    for s in range(12):\n",
        "      if not np.isnan(self.policy[s]):\n",
        "        self.states = np.zeros((1,12))\n",
        "        self.states[0,s] = 1.0\n",
        "        action = int(self.policy[s])\n",
        "        self.values[s] = self.rewards[s] + self.gamma * np.sum(np.multiply(self.values, np.dot(self.states, self.transits[:,:,action])))\n",
        "    return self.values\n",
        "\n",
        "  def expected_action(self):\n",
        "      \"\"\"Return the expected action.\n",
        "\n",
        "      It returns an action based on the\n",
        "      expected utility of doing a in state s, \n",
        "      according to T and u. This action is\n",
        "      the one that maximize the expected\n",
        "      utility.\n",
        "\n",
        "      @return expected action (int)\n",
        "      \"\"\"\n",
        "      actions = np.zeros(4)\n",
        "      for action in range(4):\n",
        "        #Expected utility of doing a in state s, according to T and u.\n",
        "        actions[action] = np.sum(np.multiply(self.values, np.dot(self.states, self.transits[:,:,action])))\n",
        "      return np.argmax(actions)\n",
        "\n",
        "def print_policy(p, shape):\n",
        "    \"\"\"Printing utility.\n",
        "\n",
        "    Print the policy actions using symbols:\n",
        "    ^, v, <, > up, down, left, right\n",
        "    * terminal states\n",
        "    # obstacles\n",
        "    \"\"\"\n",
        "    counter = 0\n",
        "    policy_string = \"\"\n",
        "    for row in range(shape[0]):\n",
        "        for col in range(shape[1]):\n",
        "            if(p[counter] == -1): policy_string += \" *  \"            \n",
        "            elif(p[counter] == 0): policy_string += \" ^  \"\n",
        "            elif(p[counter] == 1): policy_string += \" <  \"\n",
        "            elif(p[counter] == 2): policy_string += \" v  \"           \n",
        "            elif(p[counter] == 3): policy_string += \" >  \"\n",
        "            elif(np.isnan(p[counter])): policy_string += \" #  \"\n",
        "            counter += 1\n",
        "        policy_string += '\\n'\n",
        "    print(policy_string)"
      ],
      "metadata": {
        "id": "vK000bcrH7IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mdp = MDP()\n",
        "\n",
        "while True:\n",
        "    mdp.iteration += 1\n",
        "    #1- Policy evaluation\n",
        "    u_old = mdp.values.copy()\n",
        "    u = mdp.policy_evaluation()\n",
        "    #Stopping criteria\n",
        "    delta = np.absolute(u - u_old).max()\n",
        "    if delta < mdp.epsilon * (1 - mdp.gamma) / mdp.gamma: break\n",
        "    for s in range(12):\n",
        "        if not np.isnan(mdp.policy[s]) and not mdp.policy[s]==-1:\n",
        "            mdp.states = np.zeros((1,12))\n",
        "            mdp.states[0,s] = 1.0\n",
        "            #2- Policy improvement\n",
        "            a = mdp.expected_action()         \n",
        "            if a != mdp.policy[s]: mdp.policy[s] = a\n",
        "    print_policy(mdp.policy, shape=(3,4))\n",
        "\n",
        "print(\"=================== FINAL RESULT ==================\")\n",
        "print(\"Iterations: \" + str(mdp.iteration))\n",
        "print(\"Delta: \" + str(delta))\n",
        "print(\"Gamma: \" + str(mdp.gamma))\n",
        "print(\"Epsilon: \" + str(mdp.epsilon))\n",
        "print(\"===================================================\")\n",
        "print(u[0:4])\n",
        "print(u[4:8])\n",
        "print(u[8:12])\n",
        "print(\"===================================================\")\n",
        "print_policy(mdp.policy, shape=(3,4))\n",
        "print(\"===================================================\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b9WvhSbIEl5",
        "outputId": "94649d77-e62f-4d88-f443-d5be153410c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " <   <   >   *  \n",
            " ^   #   <   *  \n",
            " ^   ^   <   <  \n",
            "\n",
            " <   >   >   *  \n",
            " ^   #   ^   *  \n",
            " >   ^   ^   <  \n",
            "\n",
            " >   >   >   *  \n",
            " ^   #   ^   *  \n",
            " >   >   ^   <  \n",
            "\n",
            " >   >   >   *  \n",
            " ^   #   ^   *  \n",
            " ^   >   ^   <  \n",
            "\n",
            " >   >   >   *  \n",
            " ^   #   ^   *  \n",
            " ^   >   ^   <  \n",
            "\n",
            " >   >   >   *  \n",
            " ^   #   ^   *  \n",
            " ^   >   ^   <  \n",
            "\n",
            " >   >   >   *  \n",
            " ^   #   ^   *  \n",
            " ^   <   ^   <  \n",
            "\n",
            " >   >   >   *  \n",
            " ^   #   ^   *  \n",
            " ^   <   ^   <  \n",
            "\n",
            " >   >   >   *  \n",
            " ^   #   ^   *  \n",
            " ^   <   ^   <  \n",
            "\n",
            " >   >   >   *  \n",
            " ^   #   ^   *  \n",
            " ^   <   <   <  \n",
            "\n",
            " >   >   >   *  \n",
            " ^   #   ^   *  \n",
            " ^   <   <   <  \n",
            "\n",
            " >   >   >   *  \n",
            " ^   #   ^   *  \n",
            " ^   <   <   <  \n",
            "\n",
            " >   >   >   *  \n",
            " ^   #   ^   *  \n",
            " ^   <   <   <  \n",
            "\n",
            " >   >   >   *  \n",
            " ^   #   ^   *  \n",
            " ^   <   <   <  \n",
            "\n",
            " >   >   >   *  \n",
            " ^   #   ^   *  \n",
            " ^   <   <   <  \n",
            "\n",
            " >   >   >   *  \n",
            " ^   #   ^   *  \n",
            " ^   <   <   <  \n",
            "\n",
            " >   >   >   *  \n",
            " ^   #   ^   *  \n",
            " ^   <   <   <  \n",
            "\n",
            " >   >   >   *  \n",
            " ^   #   ^   *  \n",
            " ^   <   <   <  \n",
            "\n",
            " >   >   >   *  \n",
            " ^   #   ^   *  \n",
            " ^   <   <   <  \n",
            "\n",
            " >   >   >   *  \n",
            " ^   #   ^   *  \n",
            " ^   <   <   <  \n",
            "\n",
            " >   >   >   *  \n",
            " ^   #   ^   *  \n",
            " ^   <   <   <  \n",
            "\n",
            "=================== FINAL RESULT ==================\n",
            "Iterations: 22\n",
            "Delta: 9.127274031017762e-08\n",
            "Gamma: 0.999\n",
            "Epsilon: 0.0001\n",
            "===================================================\n",
            "[0.80796344 0.86539911 0.91653199 1.        ]\n",
            "[ 0.75696624  0.          0.65836281 -1.        ]\n",
            "[0.69968295 0.64882105 0.60471972 0.38150427]\n",
            "===================================================\n",
            " >   >   >   *  \n",
            " ^   #   ^   *  \n",
            " ^   <   <   <  \n",
            "\n",
            "===================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework: The value iteration algorithm\n",
        "\n",
        "Your task is to solve the Robot Cleaning problem using the Value Iteration algorithm.\n",
        "\n",
        "1- Based on your understanding write down the pseudo-code for Thompson sampling algorithm.\n",
        "\n",
        "2- Complete the implementation for value iteration algorithm. Basically you need to only complete the `MDP.state_utility()` method, comment on your results.\n",
        "\n",
        "3- Make sure to provide your detailed explanation for the result.\n",
        "\n",
        "4- Explain in detail the results you got in comparison with the Policy Iteration algorithm in your own words based on your understanding.\n",
        "\n",
        "\n",
        "**Note:** Your grade will be based on your understanding of the algorithm and your analysis to the results. Cheating will be punished by 50% deduction for the first time and will get 0 in the second time."
      ],
      "metadata": {
        "id": "wlKIE0U37MsR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Ss26PsG74-hR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class MDP:\n",
        "  def __init__(self):\n",
        "        #Starting state vector\n",
        "        #The agent starts from (1, 1)\n",
        "        self.states = np.array([[0.0, 0.0, 0.0, 0.0, \n",
        "                                    0.0, 0.0, 0.0, 0.0, \n",
        "                                    1.0, 0.0, 0.0, 0.0]])\n",
        "        self.rewards = np.array([-0.04, -0.04, -0.04,  +1.0,\n",
        "                                 -0.04,   0.0, -0.04,  -1.0,\n",
        "                                 -0.04, -0.04, -0.04, -0.04])\n",
        "        self.num_states = 12\n",
        "        \n",
        "        # Probabilities Transition matrix loaded from file\n",
        "        #(It is too big to write here)\n",
        "        self.transits = np.load(\"T.npy\")\n",
        "        #Generate the first policy randomly\n",
        "        # Nan=Nothing, -1=Terminal, 0=Up, 1=Left, 2=Down, 3=Right\n",
        "        self.policy = np.random.randint(0, 4, size=(12)).astype(np.float32)\n",
        "        self.policy[5] = np.NaN\n",
        "        self.policy[3] = self.policy[7] = -1\n",
        "\n",
        "        #Utility vector\n",
        "        self.values = np.array([0.0, 0.0, 0.0,  0.0,\n",
        "                                0.0, 0.0, 0.0,  0.0,\n",
        "                                0.0, 0.0, 0.0,  0.0])\n",
        "        self.gamma = 0.999\n",
        "\n",
        "        self.epsilon = 0.0001\n",
        "        self.iteration = 0\n",
        "  def state_utility(self, s):\n",
        "      \"\"\"Return the state utility.\n",
        "\n",
        "      @return the utility of the state\n",
        "      \"\"\"\n",
        "      # <Write Your Code Here>\n",
        "      pass\n",
        "      \n",
        "  def expected_action(self):\n",
        "      \"\"\"Return the expected action.\n",
        "\n",
        "      It returns an action based on the\n",
        "      expected utility of doing a in state s, \n",
        "      according to T and u. This action is\n",
        "      the one that maximize the expected\n",
        "      utility.\n",
        "\n",
        "      @return expected action (int)\n",
        "      \"\"\"\n",
        "      actions = np.zeros(4)\n",
        "      for action in range(4):\n",
        "        #Expected utility of doing a in state s, according to T and u.\n",
        "        actions[action] = np.sum(np.multiply(self.values, np.dot(self.states, self.transits[:,:,action])))\n",
        "      return np.argmax(actions)\n",
        "\n",
        "def generate_graph(utility_list):\n",
        "    \"\"\"Given a list of utility arrays (one for each iteration)\n",
        "       it generates a matplotlib graph and save it as 'output.jpg'\n",
        "    \"\"\"\n",
        "    name_list = ('(1,3)', '(2,3)', '(3,3)', '+1', '(1,2)', '#', '(3,2)', '-1', '(1,1)', '(2,1)', '(3,1)', '(4,1)')\n",
        "    color_list = ('cyan', 'teal', 'blue', 'green', 'magenta', 'black', 'yellow', 'red', 'brown', 'pink', 'gray', 'sienna')\n",
        "    counter = 0\n",
        "    index_vector = np.arange(len(utility_list))\n",
        "    for state in range(12):\n",
        "        state_list = list()\n",
        "        for utility_array in utility_list:\n",
        "             state_list.append(utility_array[state])\n",
        "        plt.plot(index_vector, state_list, color=color_list[state], label=name_list[state])  \n",
        "        counter += 1\n",
        "    #Adjust the legend and the axis\n",
        "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, 0.4), ncol=3, fancybox=True, shadow=True)\n",
        "    plt.ylim((-1.1, +1.1))\n",
        "    plt.xlim((1, len(utility_list)-1))\n",
        "    plt.ylabel('Utility', fontsize=15)\n",
        "    plt.xlabel('Iterations', fontsize=15)\n",
        "    plt.savefig(\"./output.jpg\", dpi=500)\n",
        "\n",
        "def print_policy(p, shape):\n",
        "    \"\"\"Printing utility.\n",
        "\n",
        "    Print the policy actions using symbols:\n",
        "    ^, v, <, > up, down, left, right\n",
        "    * terminal states\n",
        "    # obstacles\n",
        "    \"\"\"\n",
        "    counter = 0\n",
        "    policy_string = \"\"\n",
        "    for row in range(shape[0]):\n",
        "        for col in range(shape[1]):\n",
        "            if(p[counter] == -1): policy_string += \" *  \"            \n",
        "            elif(p[counter] == 0): policy_string += \" ^  \"\n",
        "            elif(p[counter] == 1): policy_string += \" <  \"\n",
        "            elif(p[counter] == 2): policy_string += \" v  \"           \n",
        "            elif(p[counter] == 3): policy_string += \" >  \"\n",
        "            elif(np.isnan(p[counter])): policy_string += \" #  \"\n",
        "            counter += 1\n",
        "        policy_string += '\\n'\n",
        "    print(policy_string)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mdp = MDP()\n",
        "#List containing the data for each iteation\n",
        "graph_list = list()\n",
        "\n",
        "while True:\n",
        "    delta = 0\n",
        "    # u = mdp.values\n",
        "    u_old = mdp.values.copy()\n",
        "    mdp.iteration += 1\n",
        "    graph_list.append(u_old)\n",
        "    for s in range(12):\n",
        "        mdp.states = np.zeros((1,mdp.num_states))\n",
        "        mdp.states[0,s] = 1.0\n",
        "        mdp.values[s] = mdp.state_utility(s)\n",
        "        u = mdp.values\n",
        "        delta = max(delta, np.abs(u[s] - u_old[s])) #Stopping criteria       \n",
        "    if delta < mdp.epsilon * (1 - mdp.gamma) / mdp.gamma:\n",
        "            print(\"=================== FINAL RESULT ==================\")\n",
        "            print(\"Iterations: \" + str(mdp.iteration))\n",
        "            print(\"Delta: \" + str(delta))\n",
        "            print(\"Gamma: \" + str(mdp.gamma))\n",
        "            print(\"Epsilon: \" + str(mdp.epsilon))\n",
        "            print(\"===================================================\")\n",
        "            print(u[0:4])\n",
        "            print(u[4:8])\n",
        "            print(u[8:12])\n",
        "            print(\"===================================================\")\n",
        "            for ss in range(12):\n",
        "              if not np.isnan(mdp.policy[ss]) and not mdp.policy[ss]==-1:\n",
        "                  mdp.states = np.zeros((1,12))\n",
        "                  mdp.states[0,ss] = 1.0\n",
        "                  #2- Policy improvement\n",
        "                  a = mdp.expected_action()         \n",
        "                  if a != mdp.policy[ss]: mdp.policy[ss] = a\n",
        "            print_policy(mdp.policy, shape=(3,4))\n",
        "            print(\"===================================================\")\n",
        "\n",
        "            break\n",
        "generate_graph(graph_list)"
      ],
      "metadata": {
        "id": "6gGsywLXVeTU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "c8b9994d-d686-431e-f07a-b104ae7e7da1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================== FINAL RESULT ==================\n",
            "Iterations: 22\n",
            "Delta: 5.578678757833799e-08\n",
            "Gamma: 0.999\n",
            "Epsilon: 0.0001\n",
            "===================================================\n",
            "[0.80796344 0.86539911 0.91653199 1.        ]\n",
            "[ 0.75696624  0.          0.65836281 -1.        ]\n",
            "[0.69968296 0.64882106 0.60471973 0.38150428]\n",
            "===================================================\n",
            " >   >   >   *  \n",
            " ^   #   ^   *  \n",
            " ^   <   <   <  \n",
            "\n",
            "===================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sFCnBZoq9WXl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}